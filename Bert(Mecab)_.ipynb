{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Bert(Mecab) .ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JunSeokCheon/Natural-Language-based-Climate-Technology-Classification/blob/master/Bert(Mecab)_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w70oB5d1twq5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92TkEPjZL-H1"
      },
      "source": [
        "! git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWOkXPCyMCs0"
      },
      "source": [
        "ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DUWX7_xAMFHf"
      },
      "source": [
        "cd Mecab-ko-for-Google-Colab/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EG7HUm5qMNBW"
      },
      "source": [
        "! bash install_mecab-ko_on_colab190912.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MiSK-3qt6Ro"
      },
      "source": [
        "pip install konlpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4np0U57uGF8"
      },
      "source": [
        "pip install transformers==3.3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YlhhU3wRh3D"
      },
      "source": [
        "from konlpy.tag import Mecab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44TFTqZyRX46"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import json\n",
        "import os\n",
        "import tqdm\n",
        "\n",
        "from konlpy.tag import Okt\n",
        "\n",
        "import sklearn\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from sklearn.metrics import log_loss, accuracy_score,f1_score\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from transformers import *\n",
        "#from transformers import BertModel, TFBertModel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLnXLyO6u4M7"
      },
      "source": [
        "cd /content/drive/MyDrive/daicon/data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyTDpJglRYep"
      },
      "source": [
        "train=pd.read_csv('preprocessing_train.csv')\n",
        "test=pd.read_csv('preprocessing_test.csv')\n",
        "sample_submission=pd.read_csv('sample_submission.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "P6XIwiwisrJC"
      },
      "source": [
        "train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "C4QSTfOMsrJC"
      },
      "source": [
        "test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3FNDKHYR6N63"
      },
      "source": [
        "print(f'train.shape:{train.shape}')\n",
        "print(f'test.shape:{test.shape}')\n",
        "print(f'train label 개수: {train.label.nunique()}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7i7acw_l6RTq"
      },
      "source": [
        "#이번 베이스라인에서는 과제명 뿐만 아니라 요약문_연구내용도 모델에 학습시켜보겠습니다.\n",
        "train=train[['사업명', '사업_부처명', '과제명', '요약문_연구목표', '요약문_한글키워드', '요약문_영문키워드', '요약문_연구내용', 'label']]\n",
        "test=test[['사업명', '사업_부처명', '과제명', '요약문_연구목표', '요약문_한글키워드', '요약문_영문키워드', '요약문_연구내용']]\n",
        "train[['사업명', '사업_부처명', '과제명', '요약문_연구목표', '요약문_한글키워드', '요약문_영문키워드', '요약문_연구내용', 'label']].fillna('NAN', inplace=True)\n",
        "test[['사업명', '사업_부처명', '과제명', '요약문_연구목표', '요약문_한글키워드', '요약문_영문키워드', '요약문_연구내용']].fillna('NAN', inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRO55TtZ6Rh0"
      },
      "source": [
        "# train['data']=train['사업명']+ '\\n' + train['사업_부처명'] + '\\n' + train['과제명'] + '\\n' + train['요약문_한글키워드'] + '\\n' + train['요약문_영문키워드']\n",
        "train['data']= train['과제명'] + '\\n' + train['요약문_한글키워드']\n",
        "# test['data']=test['사업명']+ '\\n' + test['사업_부처명'] + '\\n' + test['과제명'] + '\\n' + test['요약문_한글키워드'] + '\\n' + test['요약문_영문키워드']\n",
        "test['data']= test['과제명'] + '\\n' + test['요약문_한글키워드']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Abr-lKBzzhA"
      },
      "source": [
        "train = train.fillna('')\n",
        "test = test.fillna('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARiDdsKh6Sht"
      },
      "source": [
        "print(train.shape)\n",
        "print(test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DRYSrZZ6Tjr"
      },
      "source": [
        "train.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XAGz5XK6U1E"
      },
      "source": [
        "test.head(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BClHRta16VoM"
      },
      "source": [
        "#random seed 고정\n",
        "tf.random.set_seed(1234)\n",
        "np.random.seed(1234)\n",
        "BATCH_SIZE = 32\n",
        "NUM_EPOCHS = 10\n",
        "VALID_SPLIT = 0.1\n",
        "MAX_LEN= 256"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QMI-iQbBtcO"
      },
      "source": [
        "pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHxm6znsPH3C"
      },
      "source": [
        "from transformers import ElectraTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "An_TnDOCxLar"
      },
      "source": [
        "from transformers import *\n",
        "tokenizer= ElectraTokenizer.from_pretrained(\"/content/drive/MyDrive/daicon/koelectra-base-v3\", do_lower_case=False)\n",
        "\n",
        "def bert_tokenizer(sent, MAX_LEN):\n",
        "    \n",
        "    encoded_dict=tokenizer.encode_plus(\n",
        "    text = sent, \n",
        "    add_special_tokens=True, \n",
        "    max_length=MAX_LEN, \n",
        "    pad_to_max_length=True, \n",
        "    return_attention_mask=True,\n",
        "    truncation = True)\n",
        "    \n",
        "    input_id=encoded_dict['input_ids']\n",
        "    attention_mask=encoded_dict['attention_mask']\n",
        "    token_type_id = encoded_dict['token_type_ids']\n",
        "    \n",
        "    return input_id, attention_mask, token_type_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PW7ybebw6Wcs"
      },
      "source": [
        "input_ids =[]\n",
        "attention_masks =[]\n",
        "token_type_ids =[]\n",
        "train_data_labels = []\n",
        "mecab = Mecab()\n",
        "\n",
        "def clean_text(sent):\n",
        "    sent_clean=re.sub(\"[^가-힣ㄱ-하-ㅣ ]\", \" \", sent)\n",
        "    word_text=mecab.morphs(sent_clean)\n",
        "    word_review=[token for token in word_text if not len(token)==1]\n",
        "    word_review = \"\\n\".join(str(w) for w in word_review)\n",
        "    return word_review\n",
        "\n",
        "for train_sent, train_label in zip(train['data'], train['label']):\n",
        "    try:\n",
        "        input_id, attention_mask, token_type_id = bert_tokenizer(clean_text(train_sent), MAX_LEN=MAX_LEN)\n",
        "        # input_id, attention_mask, token_type_id = bert_tokenizer(train_sent, MAX_LEN=MAX_LEN)\n",
        "        \n",
        "        input_ids.append(input_id)\n",
        "        attention_masks.append(attention_mask)\n",
        "        token_type_ids.append(token_type_id)\n",
        "        #########################################\n",
        "        train_data_labels.append(train_label)\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        print(train_sent)\n",
        "        pass\n",
        "\n",
        "train_input_ids=np.array(input_ids, dtype=int)\n",
        "train_attention_masks=np.array(attention_masks, dtype=int)\n",
        "train_token_type_ids=np.array(token_type_ids, dtype=int)\n",
        "###########################################################\n",
        "train_inputs=(train_input_ids, train_attention_masks, train_token_type_ids)\n",
        "train_labels=np.asarray(train_data_labels, dtype=np.int32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuEOpmz46Xdg",
        "scrolled": true
      },
      "source": [
        "print(train_input_ids[1])\n",
        "print(train_attention_masks[1])\n",
        "print(train_token_type_ids[1])\n",
        "print(tokenizer.decode(train_input_ids[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swRDcyNj6Yv2"
      },
      "source": [
        "class TFBertClassifier(tf.keras.Model):\n",
        "    def __init__(self, model_name, dir_path, num_class):\n",
        "        super(TFBertClassifier, self).__init__()\n",
        "\n",
        "        self.bert = TFBertModel.from_pretrained(model_name, cache_dir=dir_path)\n",
        "        self.dropout = tf.keras.layers.Dropout(self.bert.config.hidden_dropout_prob)\n",
        "        self.classifier = tf.keras.layers.Dense(num_class, \n",
        "                                                kernel_initializer=tf.keras.initializers.TruncatedNormal(self.bert.config.initializer_range), \n",
        "                                                name=\"classifier\")\n",
        "        \n",
        "    def call(self, inputs, attention_mask=None, token_type_ids=None, training=False):\n",
        "        \n",
        "        #outputs 값: # sequence_output, pooled_output, (hidden_states), (attentions)\n",
        "        outputs = self.bert(inputs, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "        pooled_output = outputs[1] \n",
        "        pooled_output = self.dropout(pooled_output, training=training)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        return logits\n",
        "\n",
        "\n",
        "cls_model = TFBertClassifier(model_name='bert-base-multilingual-cased', dir_path='bert_ckpt', num_class=46)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qygduVZF58oU",
        "scrolled": true
      },
      "source": [
        "# 학습 준비하기\n",
        "optimizer = tf.keras.optimizers.Adam(3e-5)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
        "cls_model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
        "\n",
        "model_name = \"tf2_bert_classifier\"\n",
        "\n",
        "# overfitting을 막기 위한 ealrystop 추가\n",
        "earlystop_callback = EarlyStopping(monitor='val_loss', min_delta=0.0001,patience=3)\n",
        "# min_delta: the threshold that triggers the termination (acc should at least improve 0.0001)\n",
        "# patience: no improvment epochs (patience = 1, 1번 이상 상승이 없으면 종료)\\\n",
        "\n",
        "checkpoint_path = os.path.join(model_name, 'weights.h5')\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# Create path if exists\n",
        "if os.path.exists(checkpoint_dir):\n",
        "    print(\"{} -- Folder already exists \\n\".format(checkpoint_dir))\n",
        "else:\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    print(\"{} -- Folder create complete \\n\".format(checkpoint_dir))\n",
        "    \n",
        "cp_callback = ModelCheckpoint(\n",
        "    checkpoint_path, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True)\n",
        "\n",
        "# 학습과 eval 시작\n",
        "model_history = cls_model.fit(train_inputs, train_labels, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, validation_split = VALID_SPLIT, callbacks=[earlystop_callback, cp_callback])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "el6vTn9bsrJJ"
      },
      "source": [
        "model_history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s3pOlcXb6aRD"
      },
      "source": [
        "input_ids =[]\n",
        "attention_masks =[]\n",
        "token_type_ids =[]\n",
        "train_data_labels = []\n",
        "\n",
        "for test_sent in test['data']:\n",
        "    try:\n",
        "        input_id, attention_mask, token_type_id = bert_tokenizer(clean_text(test_sent), MAX_LEN=MAX_LEN)\n",
        "        # input_id, attention_mask, token_type_id = bert_tokenizer(test_sent, MAX_LEN=MAX_LEN)\n",
        "        \n",
        "        input_ids.append(input_id)\n",
        "        attention_masks.append(attention_mask)\n",
        "        token_type_ids.append(token_type_id)\n",
        "        #########################################\n",
        "       \n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        print(test_sent)\n",
        "        pass\n",
        "    \n",
        "test_input_ids=np.array(input_ids, dtype=int)\n",
        "test_attention_masks=np.array(attention_masks, dtype=int)\n",
        "test_token_type_ids=np.array(token_type_ids, dtype=int)\n",
        "###########################################################\n",
        "test_inputs=(test_input_ids, test_attention_masks, test_token_type_ids)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8TvtGOWQ6Zye"
      },
      "source": [
        "results = cls_model.predict(test_inputs)\n",
        "results=tf.argmax(results, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CgGtLAhAEcz"
      },
      "source": [
        "results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "68N_Fk-M4b1V"
      },
      "source": [
        "sample_submission['label']=results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bw35Vuu3srJK"
      },
      "source": [
        "sample_submission.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0zsDD5_zci1D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e3f11d9-99a1-4dca-d073-94c3bb7e7ab6"
      },
      "source": [
        "cd /content/drive/MyDrive/daicon/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/daicon\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFJLUX4s3ul_"
      },
      "source": [
        "sample_submission.to_csv('submission/[과제명&요약문_한글키워드],  koelectra vocab, maxlen256, split0.1, mecab, 10epoch - 21.8.15.csv', index=None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUHgqGd6srJK"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(model_history.history['accuracy'])\n",
        "plt.plot(model_history.history['val_accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}